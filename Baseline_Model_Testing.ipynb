{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"colab":{"name":"FaceMaskDetectionModelTesting.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"bf5e684c"},"source":["import pickle\n","import os\n","import cv2\n","import numpy as np"],"id":"bf5e684c","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KU3fT0eVQBDe","executionInfo":{"status":"ok","timestamp":1639138251916,"user_tz":-330,"elapsed":23425,"user":{"displayName":"Meenal Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisO-bKxbyT0bFOmAZ3gGP-ElWSnpJ51FQD2WaQ=s64","userId":"05829239071734237059"}},"outputId":"21cfada8-6a44-4c41-a9df-1316aa4ee1ea"},"id":"KU3fT0eVQBDe","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"73e4515b"},"source":["haar_data = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")"],"id":"73e4515b","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ef03aea9"},"source":["img_size=200\n","def TestModel(path, label, model):\n","    accuracy = 0\n","    images = os.listdir(path)\n","    faceData=[]\n","    test_y = []\n","    for img in images:\n","        img_path = os.path.join(path,img)\n","        image = cv2.imread(img_path)\n","        image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n","        faces = haar_data.detectMultiScale(image)\n","        if len(faces) == 0:\n","            face = cv2.resize(image,(img_size,img_size))\n","            face = np.array(face).flatten()\n","            faceData.append(face)\n","            test_y.append([label])\n","        else:\n","            for x,y,w,h in faces:\n","                face = image[y:y+h, x:x+w]\n","                face = cv2.resize(image,(img_size,img_size))\n","                face = np.array(face).flatten()\n","                faceData.append(face)\n","                test_y.append([label])\n","             \n","    pred_y = model.predict(faceData) \n","    return np.mean(pred_y==test_y)"],"id":"ef03aea9","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8dc76297"},"source":["path_masked = '/content/drive/MyDrive/Mask Dataset/with_mask'\n","path_unmasked = '/content/drive/MyDrive/Mask Dataset/without_mask'"],"id":"8dc76297","execution_count":null,"outputs":[]},{"cell_type":"code","source":["pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"bgFvCU7sPp1Z","executionInfo":{"status":"ok","timestamp":1639138274517,"user_tz":-330,"elapsed":568,"user":{"displayName":"Meenal Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisO-bKxbyT0bFOmAZ3gGP-ElWSnpJ51FQD2WaQ=s64","userId":"05829239071734237059"}},"outputId":"a8f414f4-ce3c-48cf-9baa-59374e20b88a"},"id":"bgFvCU7sPp1Z","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content'"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"3fc1cf02","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639138607565,"user_tz":-330,"elapsed":10915,"user":{"displayName":"Meenal Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisO-bKxbyT0bFOmAZ3gGP-ElWSnpJ51FQD2WaQ=s64","userId":"05829239071734237059"}},"outputId":"536a6918-068b-4d68-cfb8-f4dcebdf35d1"},"source":["#Loadinbg the saved models\n","filename_dt = '/content/drive/MyDrive/FaceDetectionJupyter/DecisionTreeModel.sav'\n","dt_model = pickle.load(open(filename_dt, 'rb'))\n","\n","filename_svm = '/content/drive/MyDrive/FaceDetectionJupyter/SVMModel.sav'\n","svm_model = pickle.load(open(filename_svm, 'rb'))\n","\n","# filename_knn = 'KNNModel.sav'\n","# knn_model = pickle.load(open(filename_knn, 'rb'))\n","\n","# filename_rf = 'RandomForest.sav'\n","# rf_model = pickle.load(open(filename_rf, 'rb'))"],"id":"3fc1cf02","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.0 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n","  UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator SVC from version 1.0 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n","  UserWarning,\n"]}]},{"cell_type":"markdown","source":["# New Section"],"metadata":{"id":"uH_IeVnCPwGv"},"id":"uH_IeVnCPwGv"},{"cell_type":"code","metadata":{"id":"e9356362","outputId":"b543737c-d92d-49f0-8dbb-6b3b861a10c3","colab":{"base_uri":"https://localhost:8080/","height":322},"executionInfo":{"status":"error","timestamp":1639138615048,"user_tz":-330,"elapsed":1150,"user":{"displayName":"Meenal Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisO-bKxbyT0bFOmAZ3gGP-ElWSnpJ51FQD2WaQ=s64","userId":"05829239071734237059"}}},"source":["accuracy = TestModel(path_masked,1, dt_model)\n","print(accuracy)"],"id":"e9356362","execution_count":null,"outputs":[{"output_type":"error","ename":"error","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-3b7756b81137>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTestModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_masked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-caa4d8a59416>\u001b[0m in \u001b[0;36mTestModel\u001b[0;34m(path, label, model)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhaar_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaces\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/objdetect/src/cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'detectMultiScale'\n"]}]},{"cell_type":"code","metadata":{"id":"877ee979","outputId":"45c30319-ba6d-4d27-c337-07e73ce893fd"},"source":["accuracy = TestModel(path_unmasked, 0, dt_model)\n","print(accuracy)"],"id":"877ee979","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["0.9640718562874252\n"]}]},{"cell_type":"code","metadata":{"id":"1b6c7bcc","outputId":"f488e79e-5fcb-4f2f-c5b8-a215661815a8"},"source":["accuracy = TestModel(path_masked,1, svm_model)\n","print(accuracy)"],"id":"1b6c7bcc","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["0.9025844930417495\n"]}]},{"cell_type":"code","metadata":{"id":"d53563dc","outputId":"008d6416-80c6-4167-e9fc-4c1e9680f33d"},"source":["accuracy = TestModel(path_unmasked, 0, svm_model)\n","print(accuracy)"],"id":"d53563dc","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["0.9700598802395209\n"]}]},{"cell_type":"code","metadata":{"id":"19042ce3","outputId":"2ed56c67-f83a-43e3-ba0e-00098416c2f1"},"source":["accuracy = TestModel(path_masked,1, knn_model)\n","print(accuracy)"],"id":"19042ce3","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["0.7852882703777336\n"]}]},{"cell_type":"code","metadata":{"id":"4546b183","outputId":"d7e302d4-dbee-4b40-c133-b64b9c91253e"},"source":["accuracy = TestModel(path_unmasked, 0, knn_model)\n","print(accuracy)"],"id":"4546b183","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["0.9640718562874252\n"]}]},{"cell_type":"code","metadata":{"id":"8fd3bd46","outputId":"29764e7e-caf9-4b24-96f7-4a8c011f74fe"},"source":["accuracy = TestModel(path_masked,1, rf_model)\n","print(accuracy)"],"id":"8fd3bd46","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["0.9602385685884692\n"]}]},{"cell_type":"code","metadata":{"id":"07e84ea0","outputId":"8c904343-bf16-4cae-84d0-62d89f7506d1"},"source":["accuracy = TestModel(path_unmasked, 0, rf_model)\n","print(accuracy)"],"id":"07e84ea0","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["0.9940119760479041\n"]}]},{"cell_type":"code","metadata":{"id":"4c2fd9b5","outputId":"1ff32319-7c01-4812-bc6b-471a64ca98bf"},"source":["img_path = 'MyDataset/masked'\n","accuracy = TestModel(img_path, 1, rf_model)\n","print(accuracy)"],"id":"4c2fd9b5","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["0.5714285714285714\n"]}]},{"cell_type":"code","metadata":{"id":"51ee4cd6","outputId":"88e05251-7440-4ae1-fa57-c2855280efe4"},"source":["img_path = 'MyDataset/masked'\n","accuracy = TestModel(img_path, 1, svm_model)\n","print(accuracy)"],"id":"51ee4cd6","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["0.8571428571428571\n"]}]},{"cell_type":"code","metadata":{"id":"c23c9e51"},"source":["def TestModelForOneImage(img_path, label, model):\n","    faceData=[]\n","    test_y = []\n","    prediction = [] \n","    image = cv2.imread(img_path)\n","    image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n","    faces = haar_data.detectMultiScale(image)\n","    if len(faces) == 0:\n","        face = cv2.resize(image,(img_size,img_size))\n","        face = np.array(face).flatten()\n","        face = np.array(face)\n","        faceData.append(face)\n","        test_y.append([label])\n","    else:\n","        for x,y,w,h in faces:\n","            face = image[y:y+h, x:x+w]\n","            face = cv2.resize(image,(img_size,img_size))\n","            face = np.array(face).flatten()\n","            face = np.array(face)\n","            faceData.append(face)\n","            test_y.append([label])\n","    \n","    pred_y = model.predict(faceData)     \n","    return pred_y"],"id":"c23c9e51","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"62991ae1"},"source":["image_path = 'MyDataset/Masked/gpic4.jpg'"],"id":"62991ae1","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"83bf1c33","outputId":"65629b2e-4764-470b-f7d8-65811aecbfdf"},"source":["accuracy = TestModelForOneImage(image_path, 1, svm_model)\n","print(accuracy)"],"id":"83bf1c33","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["[1]\n"]}]},{"cell_type":"code","metadata":{"id":"d29e9055"},"source":["test_path_masked = 'MyDataset/Masked'"],"id":"d29e9055","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f898ae24","outputId":"32d07248-b825-43b9-f083-ccdcf445b67e"},"source":["accuracy = TestModel(test_path_masked, 1, knn_model)\n","print(accuracy)"],"id":"f898ae24","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["0.42857142857142855\n"]}]},{"cell_type":"code","metadata":{"id":"c203d7ea"},"source":["test_path_unmasked = 'MyDataset/Unmasked'"],"id":"c203d7ea","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ec1719b","outputId":"34c16cc9-f73c-44d4-c23c-14d8adf5bdb8"},"source":["accuracy = TestModel(test_path_unmasked, 0, knn_model)\n","print(accuracy)"],"id":"4ec1719b","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["0.3333333333333333\n"]}]},{"cell_type":"code","metadata":{"id":"b521ba90"},"source":[""],"id":"b521ba90","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c86eef5f","colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"status":"error","timestamp":1639139255918,"user_tz":-330,"elapsed":449,"user":{"displayName":"Meenal Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisO-bKxbyT0bFOmAZ3gGP-ElWSnpJ51FQD2WaQ=s64","userId":"05829239071734237059"}},"outputId":"59f7a51b-fbc0-438f-9677-eeb2cc9ca913"},"source":["# define a video capture object\n","vid = cv2.VideoCapture(0)\n","  \n","while(True):\n","      \n","    # Capture the video frame\n","    # by frame\n","    ret, frame = vid.read()\n","  \n","    # Display the resulting frame\n","    cv2.imshow('frame', frame)\n","      \n","    # the 'q' button is set as the\n","    # quitting button you may use any\n","    # desired button of your choice\n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        break\n","  \n","# After the loop release the cap object\n","vid.release()\n","# Destroy all the windows\n","cv2.de"],"id":"c86eef5f","execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-9f425bdeb6f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Display the resulting frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# the 'q' button is set as the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/patches/__init__.py\u001b[0m in \u001b[0;36mcv2_imshow\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \"\"\"\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m   \u001b[0;31m# cv2 stores colors as BGR; convert to RGB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'clip'"]}]},{"cell_type":"code","metadata":{"id":"eaa2f8d3","colab":{"base_uri":"https://localhost:8080/","height":306},"executionInfo":{"status":"error","timestamp":1639139293965,"user_tz":-330,"elapsed":2859,"user":{"displayName":"Meenal Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisO-bKxbyT0bFOmAZ3gGP-ElWSnpJ51FQD2WaQ=s64","userId":"05829239071734237059"}},"outputId":"9cdd6585-f134-48aa-8080-0b6b854c3dd7"},"source":["import cv2\n","import os\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","import numpy as np\n","import pickle\n"," \n","cascPath = os.path.dirname(\n","    cv2.__file__) + \"/data/haarcascade_frontalface_default.xml\"\n","faceCascade = cv2.CascadeClassifier(cascPath)\n","filename_rf = '/content/drive/MyDrive/FaceDetectionJupyter/SVMModel.sav'\n","model = pickle.load(open(filename_rf, 'rb'))\n"," \n","video_capture = cv2.VideoCapture(0)\n","while(True):\n","    # Capture frame-by-frame\n","    ret, frame = video_capture.read()\n","    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","    faces = faceCascade.detectMultiScale(gray,\n","                                         scaleFactor=1.1,\n","                                         minNeighbors=5,\n","                                         minSize=(60, 60),\n","                                         flags=cv2.CASCADE_SCALE_IMAGE)\n","    faces_list=[]\n","    preds=[]\n","    for (x, y, w, h) in faces:\n","        face_frame = frame[y:y+h,x:x+w]\n","        face_frame = cv2.cvtColor(face_frame, cv2.COLOR_BGR2GRAY)\n","        face_frame = cv2.resize(face_frame, (200, 200))\n","        #face_frame = img_to_array(face_frame)\n","        #face_frame = np.expand_dims(face_frame, axis=0)\n","        #face_frame =  preprocess_input(face_frame)\n","        faces_list.append(face_frame.flatten())\n","        if len(faces_list)>0:\n","            preds = model.predict(faces_list)\n","        for pred in preds:\n","            if pred==1:\n","                pred2 = (1,0)\n","            else:\n","                pred2 = (0,1)\n","            (mask, withoutMask) = pred2\n","        label = \"Mask\" if mask > withoutMask else \"No Mask\"\n","        color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n","        label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n","        cv2.putText(frame, label, (x, y- 10),\n","                    cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n"," \n","        cv2.rectangle(frame, (x, y), (x + w, y + h),color, 2)\n","        # Display the resulting frame\n","    cv2.imshow('Video', frame)\n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        break\n","video_capture.release()\n","cv2.destroyAllWindows()"],"id":"eaa2f8d3","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator SVC from version 1.0 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n","  UserWarning,\n"]},{"output_type":"error","ename":"error","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-8f61986e6b12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Capture frame-by-frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_capture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     faces = faceCascade.detectMultiScale(gray,\n\u001b[1;32m     21\u001b[0m                                          \u001b[0mscaleFactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"]}]},{"cell_type":"code","metadata":{"id":"1b0cabbd"},"source":[""],"id":"1b0cabbd","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d7a62bc7"},"source":[""],"id":"d7a62bc7","execution_count":null,"outputs":[]}]}